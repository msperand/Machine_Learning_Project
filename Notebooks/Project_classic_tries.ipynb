{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iANKWSO9KHiD"
      },
      "source": [
        "# Initialisation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNH_1HCFXuWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f5a66a3-ca58-4d13-add6-4dd3dcd629df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/235.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "\n",
        "# Import required packages\n",
        "from transformers import pipeline, DataCollatorWithPadding\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from google.colab import files\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from google.colab import files\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.semi_supervised import SelfTrainingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import xgboost as xgb\n",
        "import torchgen\n",
        "import string\n",
        "import unidecode\n",
        "import nltk\n",
        "import torch\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvyPpHR0X6Mn",
        "outputId": "756f6e4d-6f05-4d0f-aba2-468c38f08019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id difficulty\n",
            "0        0         A1\n",
            "1        1         A1\n",
            "2        2         A1\n",
            "3        3         A1\n",
            "4        4         A1\n",
            "...    ...        ...\n",
            "1195  1195         A1\n",
            "1196  1196         A1\n",
            "1197  1197         A1\n",
            "1198  1198         A1\n",
            "1199  1199         A1\n",
            "\n",
            "[1200 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# URL to the raw CSV file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/msperand/Machine_Learning_Project/main/Data/sample_submission.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "sample_submission = pd.read_csv(url)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(sample_submission)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjkxtWLI486G",
        "outputId": "a7535802-f91c-4c1f-caf8-244f82fd8a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                           sentence difficulty\n",
            "0        0  Les coûts kilométriques réels peuvent diverger...         C1\n",
            "1        1  Le bleu, c'est ma couleur préférée mais je n'a...         A1\n",
            "2        2  Le test de niveau en français est sur le site ...         A1\n",
            "3        3           Est-ce que ton mari est aussi de Boston?         A1\n",
            "4        4  Dans les écoles de commerce, dans les couloirs...         B1\n",
            "...    ...                                                ...        ...\n",
            "4795  4795  C'est pourquoi, il décida de remplacer les hab...         B2\n",
            "4796  4796  Il avait une de ces pâleurs splendides qui don...         C1\n",
            "4797  4797  Et le premier samedi de chaque mois, venez ren...         A2\n",
            "4798  4798  Les coûts liés à la journalisation n'étant pas...         C2\n",
            "4799  4799  Sur le sable, la mer haletait de toute la resp...         C2\n",
            "\n",
            "[4800 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# URL to the raw CSV file on GitHub\n",
        "url2 = 'https://raw.github.com/msperand/Machine_Learning_Project/main/Data/training_data.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "training_data = pd.read_csv(url2)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY4Prj815FOQ",
        "outputId": "4c17dec1-acad-4e16-a8fa-63518825f9b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id                                           sentence\n",
            "0        0  Nous dûmes nous excuser des propos que nous eû...\n",
            "1        1  Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
            "2        2  Et, paradoxalement, boire froid n'est pas la b...\n",
            "3        3  Ce n'est pas étonnant, car c'est une saison my...\n",
            "4        4  Le corps de Golo lui-même, d'une essence aussi...\n",
            "...    ...                                                ...\n",
            "1195  1195  C'est un phénomène qui trouve une accélération...\n",
            "1196  1196  Je vais parler au serveur et voir si on peut d...\n",
            "1197  1197  Il n'était pas comme tant de gens qui par pare...\n",
            "1198  1198      Ils deviennent dangereux pour notre économie.\n",
            "1199  1199  Son succès a généré beaucoup de réactions néga...\n",
            "\n",
            "[1200 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# URL to the raw CSV file on GitHub\n",
        "url3 = 'https://raw.github.com/msperand/Machine_Learning_Project/main/Data/unlabelled_test_data.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "test_data = pd.read_csv(url3)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu7cvZFP7JIH"
      },
      "source": [
        "# Knowing the data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRgnbHVB0RS6",
        "outputId": "c264b12d-0274-4ed7-e24a-e2e4f0cbf04f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary Statistics of Sentence Lengths:\n",
            " count    1200.000000\n",
            "mean      109.152500\n",
            "std        96.488078\n",
            "min         6.000000\n",
            "25%        49.000000\n",
            "50%        81.000000\n",
            "75%       144.000000\n",
            "max       930.000000\n",
            "Name: sentence_length, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Assuming 'test_data' contains a column named 'sentence' which holds the text data\n",
        "# Calculate the length of each sentence\n",
        "test_data['sentence_length'] = test_data['sentence'].apply(len)\n",
        "\n",
        "# Display summary statistics for the sentence lengths\n",
        "sentence_length_stats = test_data['sentence_length'].describe()\n",
        "\n",
        "# Print the summary statistics\n",
        "print(\"Summary Statistics of Sentence Lengths:\\n\", sentence_length_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX6Q8xWA1XaE",
        "outputId": "a2ad2db5-ec20-4330-daf9-fb747891db81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary Statistics of Sentence Lengths:\n",
            " count    4800.000000\n",
            "mean      110.355208\n",
            "std       100.873552\n",
            "min         4.000000\n",
            "25%        49.000000\n",
            "50%        84.000000\n",
            "75%       142.000000\n",
            "max      1572.000000\n",
            "Name: sentence_length, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Calculate the length of each sentence\n",
        "training_data['sentence_length'] = training_data['sentence'].apply(len)\n",
        "\n",
        "# Display summary statistics for the sentence lengths\n",
        "sentence_length_stats = training_data['sentence_length'].describe()\n",
        "\n",
        "# Print the summary statistics\n",
        "print(\"Summary Statistics of Sentence Lengths:\\n\", sentence_length_stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lgZfo572peO",
        "outputId": "408ceb4a-88ab-4b1d-984a-81eb804f3a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated training_data with sentences of 1000 characters or less:\n",
            "   id                                           sentence difficulty  \\\n",
            "0   0  Les coûts kilométriques réels peuvent diverger...         C1   \n",
            "1   1  Le bleu, c'est ma couleur préférée mais je n'a...         A1   \n",
            "2   2  Le test de niveau en français est sur le site ...         A1   \n",
            "3   3           Est-ce que ton mari est aussi de Boston?         A1   \n",
            "4   4  Dans les écoles de commerce, dans les couloirs...         B1   \n",
            "\n",
            "   sentence_length  \n",
            "0              255  \n",
            "1               62  \n",
            "2               66  \n",
            "3               40  \n",
            "4              209  \n",
            "\n",
            "Number of sentences removed: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ade5356ceb63>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  training_data.drop(columns=['char_count'], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Count the number of characters in each sentence\n",
        "training_data['char_count'] = training_data['sentence'].apply(len)\n",
        "\n",
        "# Filter out sentences with more than 1000 characters\n",
        "original_count = len(training_data)  # Store the original number of sentences for comparison\n",
        "training_data = training_data[training_data['char_count'] <= 1000]\n",
        "\n",
        "# Drop the 'char_count' column as it is no longer needed\n",
        "training_data.drop(columns=['char_count'], inplace=True)\n",
        "\n",
        "# Display the first few rows of the updated DataFrame to ensure it's processed correctly\n",
        "print(\"Updated training_data with sentences of 1000 characters or less:\")\n",
        "print(training_data.head())\n",
        "\n",
        "# Display the number of sentences removed\n",
        "print(\"\\nNumber of sentences removed:\", original_count - len(training_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dummy tries"
      ],
      "metadata": {
        "id": "w__a6FdGzMRd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YsnHLUT7V7W",
        "outputId": "acafd0e3-78be-4f6a-8ff5-d5cb2fa33472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id difficulty\n",
            "0        0         A2\n",
            "1        1         A2\n",
            "2        2         A2\n",
            "3        3         A2\n",
            "4        4         A2\n",
            "...    ...        ...\n",
            "1195  1195         A2\n",
            "1196  1196         A2\n",
            "1197  1197         A2\n",
            "1198  1198         A2\n",
            "1199  1199         A2\n",
            "\n",
            "[1200 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# Assuming 'sample_submission' is your DataFrame\n",
        "sample_submission['difficulty'] = 'A2'\n",
        "\n",
        "# Now the 'difficulty' column should be updated to 'A2' for all rows\n",
        "# Display the updated DataFrame to confirm the changes\n",
        "print(sample_submission)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM06-61r7rKD"
      },
      "outputs": [],
      "source": [
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_A2.csv'  # Temporary file name in the Colab environment\n",
        "sample_submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05nl9njk8ks0"
      },
      "outputs": [],
      "source": [
        "# Assuming 'sample_submission' is your DataFrame which has been updated\n",
        "sample_submission['difficulty'] = 'B1'  # Updating the difficulty column to 'A2'\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_B1.csv'  # Temporary file name in the Colab environment\n",
        "sample_submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZTrN8MV8uVp"
      },
      "outputs": [],
      "source": [
        "# Assuming 'sample_submission' is your DataFrame which has been updated\n",
        "sample_submission['difficulty'] = 'B2'  # Updating the difficulty column to 'A2'\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_B2.csv'  # Temporary file name in the Colab environment\n",
        "sample_submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bH2BJnz8yDH"
      },
      "outputs": [],
      "source": [
        "# Assuming 'sample_submission' is your DataFrame which has been updated\n",
        "sample_submission['difficulty'] = 'C1'  # Updating the difficulty column to 'A2'\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_C1.csv'  # Temporary file name in the Colab environment\n",
        "sample_submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyUu3V5181Ki"
      },
      "outputs": [],
      "source": [
        "# Assuming 'sample_submission' is your DataFrame which has been updated\n",
        "sample_submission['difficulty'] = 'C2'  # Updating the difficulty column to 'A2'\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_C2.csv'  # Temporary file name in the Colab environment\n",
        "sample_submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic tries"
      ],
      "metadata": {
        "id": "yuA5S1dzzhjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqJwywnCASLx"
      },
      "outputs": [],
      "source": [
        "# Split the training data\n",
        "X_train = training_data['sentence']\n",
        "y_train = training_data['difficulty']\n",
        "\n",
        "# Create a TF-IDF vectorizer and Naive Bayes classifier pipeline\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the difficulty for the test data\n",
        "predictions = model.predict(test_data['sentence'])\n",
        "\n",
        "# Create a DataFrame with the ID and predicted difficulty\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'difficulty': predictions\n",
        "})\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_1.csv'  # Temporary file name in the Colab environment\n",
        "submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRwCEy7miNlo",
        "outputId": "58bb28df-493e-4031-fa61-9ec6b2aec810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy pipeline: 0.4358706986444213\n"
          ]
        }
      ],
      "source": [
        "# Split the data into features (X) and labels (y)\n",
        "X = training_data['sentence']\n",
        "y = training_data['difficulty']\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy pipeline:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf1pUzvED7m8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebd4084a-3982-44db-a03e-845786da2aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load French stop words\n",
        "french_stop_words = set(stopwords.words('french'))\n",
        "\n",
        "# Define a function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Normalize accents and remove non-ASCII characters\n",
        "    text = unidecode.unidecode(text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    words = [word for word in words if word not in french_stop_words]\n",
        "    # Rejoin the words back into one string\n",
        "    return \" \".join(words)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxfohF85En89",
        "outputId": "78a707fb-902c-46df-de52-1a0a825d253e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lintelligence artificielle letude agents induits machines\n"
          ]
        }
      ],
      "source": [
        "# Test the preprocessing function\n",
        "sample_text = \"L'intelligence artificielle est l'étude des agents induits par des machines.\"\n",
        "preprocessed_text = preprocess_text(sample_text)\n",
        "print(preprocessed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWp2OomcFYhH"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing to the training data\n",
        "training_data_preprocessed = training_data\n",
        "training_data_preprocessed['sentence'] = training_data['sentence'].apply(preprocess_text)\n",
        "\n",
        "# Apply the preprocessing to the test data\n",
        "test_data_preprocessed = test_data\n",
        "test_data_preprocessed['sentence'] = test_data['sentence'].apply(preprocess_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T-bYiT2CCjTh",
        "outputId": "da0b6c74-235c-4702-e8d4-6e301c68cd56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy pipeline with preprocessing: 0.4056308654848801\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_98ceb535-04e5-41a1-9843-e14f9dec2f91\", \"sample_submission_2.csv\", 8504)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Preprocess the text if necessary (e.g., lowercasing, stemming)\n",
        "# For simplicity, we'll assume the data is clean and well-formatted for now\n",
        "\n",
        "# Split the training data into a training set and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(training_data_preprocessed['sentence'], training_data_preprocessed['difficulty'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a pipeline with a TF-IDF vectorizer and a Logistic Regression classifier\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('classifier', LogisticRegression(solver='liblinear')),\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "val_predictions = pipeline.predict(X_val)\n",
        "\n",
        "# Calculate the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "\n",
        "print(f'Validation Accuracy pipeline with preprocessing: {val_accuracy}')\n",
        "\n",
        "# Now, let's predict on the actual test data\n",
        "\n",
        "# Predict the difficulty for the test data\n",
        "test_predictions = pipeline.predict(test_data_preprocessed['sentence'])\n",
        "\n",
        "# Create a DataFrame with the ID and predicted difficulty\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data_preprocessed['id'],\n",
        "    'difficulty': test_predictions\n",
        "})\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_2.csv'  # Temporary file name in the Colab environment\n",
        "submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU99odTFIi6r",
        "outputId": "e82eac64-036b-4284-d492-552882955ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy grid search: 0.41605839416058393\n"
          ]
        }
      ],
      "source": [
        "# Split the preprocessed training data into a training set and a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    training_data_preprocessed['sentence'],\n",
        "    training_data_preprocessed['difficulty'],\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create a pipeline with a TF-IDF vectorizer and a Support Vector Classifier\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('classifier', SVC(kernel='linear'))\n",
        "])\n",
        "\n",
        "# We can also use GridSearchCV to find the best parameters for our SVC\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train the model using grid search to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the validation set with the best model\n",
        "val_predictions = best_model.predict(X_val)\n",
        "\n",
        "# Calculate the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "print(f'Validation Accuracy grid search: {val_accuracy}')\n",
        "\n",
        "# Predict the difficulty for the preprocessed test data\n",
        "test_predictions = best_model.predict(test_data_preprocessed['sentence'])\n",
        "\n",
        "# Create a DataFrame with the ID and predicted difficulty\n",
        "submission_grid_search = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'difficulty': test_predictions\n",
        "})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Zwd-T0xWJkDb",
        "outputId": "decfe9a7-36b4-406e-e199-fc81d6ea71b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_60b0fd67-1f4b-48b7-8da0-b7415f491e57\", \"sample_submission_GS.csv\", 8504)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Assuming grid_search was already done and best_model contains the best estimator\n",
        "\n",
        "# Retrieve the best parameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Extract the best value of C parameter for SVC\n",
        "best_C = best_params['classifier__C']\n",
        "\n",
        "# Now re-create the pipeline with the best parameters and retrain on the entire training data\n",
        "final_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('classifier', SVC(kernel='linear', C=best_C))\n",
        "])\n",
        "\n",
        "# Fit the final model on the entire training data set\n",
        "final_pipeline.fit(training_data_preprocessed['sentence'], training_data_preprocessed['difficulty'])\n",
        "\n",
        "# Now predict on the test data\n",
        "final_predictions = final_pipeline.predict(test_data_preprocessed['sentence'])\n",
        "\n",
        "# Create the final submission DataFrame\n",
        "final_submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'difficulty': final_predictions\n",
        "})\n",
        "# Save the updated DataFrame to a CSV file\n",
        "file_name = 'sample_submission_GS.csv'  # Temporary file name in the Colab environment\n",
        "final_submission.to_csv(file_name, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data = train_test_split(\n",
        "    training_data,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "QhK9AkFhzlEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFCB-wyFQb6_"
      },
      "outputs": [],
      "source": [
        "# Split the training data\n",
        "X_train = train_data['sentence']\n",
        "y_train = train_data['difficulty']\n",
        "\n",
        "# Create a TF-IDF vectorizer and Naive Bayes classifier pipeline\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the difficulty for the test data\n",
        "predictions = model.predict(val_data['sentence'])\n",
        "\n",
        "# Create a DataFrame with the ID and predicted difficulty\n",
        "pred = pd.DataFrame({\n",
        "    'id': val_data['id'],\n",
        "    'difficulty': predictions\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOrvNdqSa24H",
        "outputId": "c82dc7d9-b66e-4899-83bb-829db51cc19d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.410844629822732\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(val_data['difficulty'], predictions)\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4wVSQ89cLQU"
      },
      "outputs": [],
      "source": [
        "# Split the training data into training and validation sets\n",
        "train_data_preprocessed, val_data_preprocessed = train_test_split(training_data_preprocessed, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKSxQICCbrEq"
      },
      "outputs": [],
      "source": [
        "# Split the training data\n",
        "X_train_preprocessed = train_data_preprocessed['sentence']\n",
        "y_train = train_data_preprocessed['difficulty']\n",
        "\n",
        "# Create a TF-IDF vectorizer and Naive Bayes classifier pipeline\n",
        "model_preprocessed = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model_preprocessed.fit(X_train, y_train)\n",
        "\n",
        "# Predict the difficulty for the test data\n",
        "predictions_preprocessed = model.predict(val_data['sentence'])\n",
        "\n",
        "# Create a DataFrame with the ID and predicted difficulty\n",
        "pred = pd.DataFrame({\n",
        "    'id': val_data['id'],\n",
        "    'difficulty': predictions\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYS4AuFlcaoz",
        "outputId": "b2536b0d-27b6-4058-a4bf-8689c458c501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.410844629822732\n"
          ]
        }
      ],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(val_data_preprocessed['difficulty'], predictions_preprocessed)\n",
        "\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1jm9KOHc068",
        "outputId": "d8774ebb-e2f2-4c56-f264-48cb0aa393db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy decision tree: 0.35140771637122004\n"
          ]
        }
      ],
      "source": [
        "# Split the data into features (X) and labels (y)\n",
        "X = training_data['sentence']\n",
        "y = training_data['difficulty']\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform on training data, transform on validation data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Define the decision tree classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy decision tree:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUScVgNodckB",
        "outputId": "1de0d028-e9fa-468d-b25a-a6a8be27b631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy knn: 0.2721584984358707\n"
          ]
        }
      ],
      "source": [
        "# Define the KNN classifier\n",
        "clf = KNeighborsClassifier(n_neighbors=37)  # You can adjust the number of neighbors\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy knn:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H13kxuYKeCXO",
        "outputId": "5d528702-1aa5-4948-db19-cfb9aa09b432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy random forest: 0.3753910323253389\n"
          ]
        }
      ],
      "source": [
        "# Define the Random Forest classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the number of estimators\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy random forest:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hKmJ3c9gb_U",
        "outputId": "00069a70-d252-4399-dd3a-288c98ac19b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy svc: 0.42127215849843586\n"
          ]
        }
      ],
      "source": [
        "# Define the SVM classifier\n",
        "clf = SVC(kernel='linear', random_state=42)  # You can choose different kernels (linear, polynomial, rbf, etc.)\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy svc:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMI74oBVgiSS",
        "outputId": "daa5a121-e2f8-41fd-cb36-3e9d4f8f593d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy gbm: 0.3983315954118874\n"
          ]
        }
      ],
      "source": [
        "# Define the XGBoost classifier\n",
        "clf = xgb.XGBClassifier(objective='multi:softmax', random_state=42)  # You can adjust other parameters\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy gbm:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQunZtoxh9rb",
        "outputId": "f2598d57-bc8f-4f9a-ffbc-808246491777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy naive bayes: 0.4369134515119917\n"
          ]
        }
      ],
      "source": [
        "# Define the Multinomial Naive Bayes classifier\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy naive bayes:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYAtuNrwmF0u",
        "outputId": "dec82db5-ac0b-4b8b-c374-b0bc540bc913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.410844629822732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/semi_supervised/_self_training.py:212: UserWarning: y contains no unlabeled samples\n",
            "  warnings.warn(\"y contains no unlabeled samples\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "# Split the data into features (X) and labels (y)\n",
        "X = training_data['sentence']\n",
        "y = training_data['difficulty']\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the labeled data into training and validation sets\n",
        "X_train_labeled, X_val, y_train_labeled, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform on labeled training data, transform on validation data\n",
        "X_train_labeled_tfidf = vectorizer.fit_transform(X_train_labeled)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Define the base classifier (Multinomial Naive Bayes)\n",
        "base_classifier = MultinomialNB()\n",
        "\n",
        "# Define the self-training classifier\n",
        "self_training_classifier = SelfTrainingClassifier(base_classifier)\n",
        "\n",
        "# Fit the self-training classifier on the labeled training data\n",
        "self_training_classifier.fit(X_train_labeled_tfidf, y_train_labeled)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = self_training_classifier.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfV7HizJmWKb",
        "outputId": "242098cf-4db6-4c60-d374-34d3cfe10ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.2940563086548488\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = training_data['sentence']\n",
        "y = training_data['difficulty']\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform on training data, transform on validation data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Define the base classifier (Decision Tree in this example)\n",
        "base_classifier = DecisionTreeClassifier(max_depth=1)  # Weak learner\n",
        "\n",
        "# Define the AdaBoost classifier\n",
        "clf = AdaBoostClassifier(base_estimator=base_classifier, n_estimators=100, random_state=42)  # You can adjust the number of estimators\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the validation data\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "7cpVH_w8nCoH",
        "outputId": "02b51691-6bca-471f-a1d9-e0464b6a1e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-713b945dc7fd>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Print the best parameters found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Define the hyperparameter space\n",
        "param_dist = {\n",
        "    'tfidf__max_features': randint(1000, 5000),  # Number of features to consider\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],      # Range of n-grams\n",
        "    'clf__n_estimators': randint(100, 1000),     # Number of trees in the forest\n",
        "    'clf__max_depth': [None] + list(np.arange(10, 110, 10)),  # Maximum depth of the tree\n",
        "    'clf__min_samples_split': randint(2, 20),     # Minimum number of samples required to split an internal node\n",
        "    'clf__min_samples_leaf': randint(1, 20),      # Minimum number of samples required to be at a leaf node\n",
        "    'clf__bootstrap': [True, False]              # Whether bootstrap samples are used when building trees\n",
        "}\n",
        "\n",
        "# Define RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the validation set\n",
        "accuracy = best_model.score(X_val, y_val)\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(model, X_val, y_val):\n",
        "    y_pred = model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred, average='weighted')\n",
        "    recall = recall_score(y_val, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# Example usage with different models:\n",
        "# Assuming models are trained and X_val_tfidf, y_val are defined\n",
        "\n",
        "results = []\n",
        "\n",
        "# List of models to evaluate\n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Logistic Regression': LogisticRegression(solver='liblinear'),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(kernel='linear', random_state=42),\n",
        "    'XGBoost': xgb.XGBClassifier(objective='multi:softmax', random_state=42),\n",
        "    'AdaBoost': AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Assume vectorizer and training split as in previous example\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "# Evaluate each model\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train_tfidf, y_train)\n",
        "    metrics = evaluate_model(model, X_val_tfidf, y_val)\n",
        "    metrics['model'] = model_name\n",
        "    results.append(metrics)\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evjf9_NS_n_h",
        "outputId": "1a17a5c7-1288-4e9f-d830-b8de56a04b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   accuracy  precision    recall        f1                model\n",
            "0  0.410845   0.428029  0.410845  0.410068          Naive Bayes\n",
            "1  0.405631   0.411018  0.405631  0.404039  Logistic Regression\n",
            "2  0.351408   0.344254  0.351408  0.345034        Decision Tree\n",
            "3  0.283629   0.350460  0.283629  0.254637                  KNN\n",
            "4  0.384776   0.381636  0.384776  0.362797        Random Forest\n",
            "5  0.416058   0.417252  0.416058  0.415141                  SVM\n",
            "6  0.387904   0.381500  0.387904  0.379298              XGBoost\n",
            "7  0.294056   0.343244  0.294056  0.283024             AdaBoost\n"
          ]
        }
      ]
    }
  ]
}